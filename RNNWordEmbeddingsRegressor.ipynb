{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNWordEmbeddingsRegressor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiXSWSvq63AEkCBJqNn7CX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/WineReviews/blob/main/RNNWordEmbeddingsRegressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkYHcdNVPCw",
        "outputId": "c62347e9-c734-479c-b1c8-a639f194d522"
      },
      "source": [
        "!git clone -l -s https://github.com/mateopolancecofficial/WineReviews.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WineReviews'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 22 (delta 9), reused 11 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDgNB4I4Wnnl",
        "outputId": "dfd7de55-36fd-4f75-e700-9d405b44d254"
      },
      "source": [
        "pip install -q -U tensorflow-text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 11.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxO8cc4cXZZS",
        "outputId": "e7b977a7-6d6b-4fa4-e5db-5ad40bd4605f"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPAeZ0j0_KfG",
        "outputId": "6d9b1c4a-97d8-486d-8d7a-c54dea1f57b3"
      },
      "source": [
        "pip install -q -U tf-models-official"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 36.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 38.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 37.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 77kB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 38.6MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ5YF_OcUp7G"
      },
      "source": [
        "## DNN regressors\n",
        "\n",
        " In this notebook we will use three different ML models. Two of them will use word embeddings for input while the last one wil use TF-IDF features\n",
        "\n",
        " First implementation\n",
        "   - transform input data into TF-IDF features and fed into ML model\n",
        "\n",
        "Second implementation\n",
        "   - create dense feature representation vector\n",
        "   - use trainable embeding layer which will create word embeddings\n",
        "   - use RNN layer to predict sentence embeddings based on given word embeddings\n",
        "   - add few Dense and Dropout layers\n",
        "  \n",
        "Third implementation\n",
        "   - use BERT preprocessing and encoder models for creating sentence embeddings\n",
        "   - add few Dense and Dropout layers\n",
        "\n",
        "All models will be fine tuned and trained with best parameters.\n",
        "\n",
        "Finally, R-squared and mean absolute error will be visualize for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diGaJU9l-wIu"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Input\n",
        ")\n",
        "import kerastuner as kt\n",
        "from official.nlp import optimization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm5vkC6_-7JK",
        "outputId": "1dfa2012-7e33-4426-c5d1-46a1c532202f"
      },
      "source": [
        "import os\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACR8UyNw__wC"
      },
      "source": [
        "### Load dataset and set global parameters\n",
        "\n",
        "Before text encoding and generating worde embeddings we need to load dataset, make train, validation and test splits and load it into tensorflow dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xec49Y-Q_WRL"
      },
      "source": [
        "# set parameters\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "path_v1 = \"/content/WineReviews/Data/winemag-data-130k-v2.csv\" \n",
        "path_v2 = \"/content/WineReviews/Data/winemag-data_first150k.csv\"\n",
        "batch_size = 2048\n",
        "col_idx = 0\n",
        "train_size, test_size, val_size = 0.8, 0.2, 0.2\n",
        "transform = 'normalize'\n",
        "columns = ['description', 'points']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnnhJzU4_gNh"
      },
      "source": [
        "def load_data(path_v1: str, path_v2: str, columns: list, col_idx: int):\n",
        "  \"\"\"\n",
        "  Load and concatenate two datasets with removing duplicates.\n",
        "  param path_v1: import path of first dataset\n",
        "  param path_v2: import path of second dataset\n",
        "  param columns: list of columns to preserve in dataframe\n",
        "  param col_idx: index of column given in input columns list \n",
        "                 on which look for duplicates in dataframe\n",
        "  return:        pandas dataframe\n",
        "  \"\"\"\n",
        "  \n",
        "  df_v1 = pd.read_csv(path_v1, index_col=0)\n",
        "  df_v1 = df_v1[columns]\n",
        "  # remove numbers form column description from first dataframe\n",
        "  df_v1.description = df_v1.description.str.replace('\\d+', '')\n",
        "\n",
        "  df_v2 = pd.read_csv(path_v2, index_col=0)\n",
        "  df_v2 = df_v2[columns]\n",
        "  # remove numbers form column description from second dataframe\n",
        "  df_v2.description = df_v2.description.str.replace('\\d+', '')\n",
        "\n",
        "  df = pd.concat([df_v1, df_v2])\n",
        "\n",
        "  # dropping duplicte values\n",
        "  df.drop_duplicates(subset = columns[col_idx],\n",
        "                       keep = 'first', inplace = True)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Suj83e_h4S"
      },
      "source": [
        "def split_data(df: pd.DataFrame, train_size: float, test_size: float, \n",
        "               val_size: float):\n",
        "  \"\"\"\n",
        "  Split dataset on train, test and validation subsets.\n",
        "  param df:          input dataframe\n",
        "  param train_size:  fraction of train size\n",
        "  param test_size:   fraction of test size\n",
        "  param val_size:    fraction of validation size\n",
        "  return:            dictionary, keys=names of dataframes, columns=dataframes\n",
        "  \"\"\"\n",
        "  \n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac = 1)\n",
        "  \n",
        "  # split on test and train set\n",
        "  text_train, text_test, y_train, y_test = train_test_split(df.description, df.points,\n",
        "                                           test_size=test_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_test = y_train.astype('float'), y_test.astype('float')\n",
        "  \n",
        "  # split train set on train and validation subsets\n",
        "  text_train, text_val, y_train, y_val = train_test_split(text_train, y_train,\n",
        "                                                  test_size=val_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_val = y_train.astype('float'), y_val.astype('float')\n",
        "\n",
        "  y_train = y_train.to_numpy().reshape(-1, 1)\n",
        "  y_val = y_val.to_numpy().reshape(-1, 1)\n",
        "  y_test = y_test.to_numpy().reshape(-1, 1)\n",
        "\n",
        "  # apply target variable transformation\n",
        "  if transform == 'normalize':\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "    \n",
        "  elif transform == 'standardize':\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "    \n",
        "  else:\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "  \n",
        "  y_train, y_val, y_test = y_train.ravel(), y_val.ravel(), y_test.ravel()\n",
        "\n",
        "  dataset_dict = {\n",
        "      'text_train': text_train,\n",
        "      'y_train': y_train,\n",
        "      'text_val': text_val,\n",
        "      'y_val': y_val,\n",
        "      'text_test': text_test,\n",
        "      'y_test': y_test,\n",
        "      'scaler': scaler\n",
        "  }\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghkiCk6A_jnj"
      },
      "source": [
        "def create_input_datasets(df_data: dict):\n",
        "  \"\"\"\n",
        "  Create tensorflow datasets based on input dataframes for train, validation \n",
        "  and test subsets.\n",
        "  param df_data: dictionary, keys=names of dataframes, columns=dataframes\n",
        "  return:        dictionary, keys=names of datasets, columns=datasets\n",
        "  \"\"\"\n",
        "\n",
        "  # create train dataset for input in tensorflow model\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_train'], \n",
        "                                                      df_data['y_train']))\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  train_ds = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_val'], \n",
        "                                                    df_data['y_val']))\n",
        "  val_dataset = val_dataset.batch(batch_size)\n",
        "  val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_test'], \n",
        "                                                     df_data['y_test']))\n",
        "  test_dataset = test_dataset.batch(batch_size)\n",
        "  test_ds = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  datasets = {\n",
        "      'train_dataset': train_dataset,\n",
        "      'val_dataset': val_dataset,\n",
        "      'test_dataset': test_dataset\n",
        "  } \n",
        "\n",
        "  return datasets"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqhH4PtbA7TV"
      },
      "source": [
        "# call data transformation functions\n",
        "df = load_data(path_v1, path_v2, columns, col_idx)\n",
        "df_data = split_data(df, train_size, test_size, val_size)\n",
        "datasets = create_input_datasets(df_data)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Rh40LVBjWV",
        "outputId": "d0a774c5-12dc-40f9-d77c-10489251cf61"
      },
      "source": [
        "for example, label in datasets['train_dataset'].take(1):\n",
        "  print('label: ', label.numpy())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label:  [0.5  0.55 0.4  ... 0.45 0.35 0.45]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ZFb3hyaIeF"
      },
      "source": [
        "# define loss functions\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def rmse():\n",
        "  def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "  return root_mean_squared_error\n",
        "\n",
        "def rmsle():\n",
        "  def root_mean_squared_log_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(K.log(1+y_pred) - K.log(1+y_true))))\n",
        "  return root_mean_squared_log_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gptTrA2xaAxk"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL5DhRAoCaFY"
      },
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(datasets['train_dataset'].map(lambda text, label: text))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGKXBwKEn1r"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def rnn_model_builder(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(encoder)\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,  return_sequences=True)))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "  hp_units_1 = hp.Int('units_1', min_value=64, max_value=128, step=16)\n",
        "  activation=hp.Choice(\n",
        "        'dense_activation',\n",
        "        values=['relu', 'tanh', 'sigmoid'],\n",
        "        default='relu'\n",
        "    )\n",
        "  model.add(Dense(units=hp_units_1, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_1',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  hp_units_2 = hp.Int('units_2', min_value=8, max_value=64, step=16)\n",
        "  model.add(Dense(units=hp_units_2, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_2',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "              loss=rmse(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKtu37R1GMKU"
      },
      "source": [
        "# define early stop callback to prevent overfitting\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhnddRT2GQAb"
      },
      "source": [
        "tuner = kt.Hyperband(rnn_model_builder,\n",
        "                     objective='mean_absolute_error',\n",
        "                     max_epochs=5,\n",
        "                     directory='RNN'\n",
        "                    )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8uSZGU4GQNr",
        "outputId": "bad0ceca-e71a-4a12-edd7-fe4fa76dda81"
      },
      "source": [
        "tuner.search(df_data['text_train'], df_data['y_train'], \n",
        "             validation_data=(df_data['text_val'], df_data['y_val']), \n",
        "             epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 9 Complete [00h 26m 55s]\n",
            "mean_absolute_error: 0.07307424396276474\n",
            "\n",
            "Best mean_absolute_error So Far: 0.0705166757106781\n",
            "Total elapsed time: 02h 21m 17s\n",
            "\n",
            "Search: Running Trial #10\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units_1           |64                |112               \n",
            "dense_activation  |tanh              |tanh              \n",
            "dropout_1         |0.4               |0.3               \n",
            "units_2           |8                 |40                \n",
            "dropout_2         |0.5               |0.2               \n",
            "learning_rate     |0.01              |0.001             \n",
            "tuner/epochs      |5                 |5                 \n",
            "tuner/initial_e...|0                 |2                 \n",
            "tuner/bracket     |0                 |1                 \n",
            "tuner/round       |0                 |1                 \n",
            "\n",
            "Epoch 1/5\n",
            "3389/3389 [==============================] - 344s 96ms/step - loss: 0.1334 - mean_absolute_error: 0.1074 - val_loss: 0.0939 - val_mean_absolute_error: 0.0744\n",
            "Epoch 2/5\n",
            "3389/3389 [==============================] - 317s 94ms/step - loss: 0.1055 - mean_absolute_error: 0.0841 - val_loss: 0.0923 - val_mean_absolute_error: 0.0731\n",
            "Epoch 3/5\n",
            " 925/3389 [=======>......................] - ETA: 3:33 - loss: 0.1045 - mean_absolute_error: 0.0832"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oiva4jXOGQa3"
      },
      "source": [
        "model_rnn = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6KU38hsDwG_",
        "outputId": "af4a07d1-e153-4da9-d37c-3c35252d7f86"
      },
      "source": [
        "# checkpoints callback is not defined because a lack of disk space on Google Colab\n",
        "history_rnn = model_rnn.fit(datasets['train_dataset'],\n",
        "                    validation_data=datasets['val_dataset'],\n",
        "                    epochs=100,\n",
        "                    callbacks=[stop_early]\n",
        "                    )"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53/53 [==============================] - 23s 270ms/step - loss: 0.3550 - mean_absolute_error: 0.3241 - val_loss: 0.1054 - val_mean_absolute_error: 0.0834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-XpjQPRaQXm"
      },
      "source": [
        "### BERT\n",
        "\n",
        "BERT model will be loaded from TensorFlow Hub and fine-tuned. There are multiple BERT models available.\n",
        "\n",
        "We will use Small BERT which have the same general architecture but fewer and/or smaller Transformer blocks than BERT Base.\n",
        "\n",
        "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba-pLD5Ee5FD"
      },
      "source": [
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGL9kJdfaTBL"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def bert_model_builder(hp):\n",
        "  \"\"\" Use and fine tune BERT model for regression task. \"\"\"\n",
        "\n",
        "  text_input = Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = Dropout(rate=hp.Float(\n",
        "                'dropout_1',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))(net)\n",
        "  net = Dense(units=1, activation=None)(net)\n",
        "  \n",
        "  model = tf.keras.Model(text_input, net)\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=rmse(),\n",
        "                metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8cmA9gbaTQ6"
      },
      "source": [
        "bert_tuner = kt.Hyperband(bert_model_builder,\n",
        "                          objective='mean_absolute_error',\n",
        "                          max_epochs=5,\n",
        "                          directory='BERT'\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2qBSIqQaTds"
      },
      "source": [
        "bert_tuner.search(df_data['text_train'], df_data['y_train'], \n",
        "                  validation_data=(df_data['text_val'], df_data['y_val']), \n",
        "                  epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "bert_best_hps = bert_tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKilu2XjkkuR"
      },
      "source": [
        "model_bert = bert_tuner.hypermodel.build(bert_best_hps)\n",
        "\n",
        "history_bert = model_bert.fit(datasets['train_dataset'],\n",
        "                              validation_data=datasets['val_dataset'],\n",
        "                              epochs=100,\n",
        "                              callbacks=[stop_early]\n",
        "                             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dmetoAIokuO"
      },
      "source": [
        "### Visualize scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zoa9GM8PpJAp"
      },
      "source": [
        "Get predictions of all models for points target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtSBZAGLkk8o"
      },
      "source": [
        "y_real = df_data['scaler'].inverse_transform(df_data['y_test'].reshape(-1, 1))\n",
        "\n",
        "rnn_result = model_rnn.predict(df_data['text_test'])\n",
        "y_predict_rnn = df_data['scaler'].inverse_transform(rnn_result.reshape(-1, 1))\n",
        "\n",
        "bert_result = model_bert.predict(df_data['text_test'])\n",
        "y_predict_bert = df_data['scaler'].inverse_transform(bert_result.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsYQD9XaklL1"
      },
      "source": [
        "estimators = {'RNN': \n",
        "              {\n",
        "                  'mae': mean_absolute_error(y_real, y_predict_rnn),\n",
        "                  'r2_score': r2_score(y_real, y_predict_rnn),\n",
        "                  'y_predict': y_predict_rnn\n",
        "              },\n",
        "              'BERT': \n",
        "              {\n",
        "                  'mae': mean_absolute_error(y_real, y_predict_bert),\n",
        "                  'r2_score': r2_score(y_real, y_predict_bert),\n",
        "                  'y_predict': y_predict_bert\n",
        "              }\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6cJgrSkD9Gn",
        "outputId": "c2d22271-3b2d-410d-bd56-ceff0dcb6b39"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "rnn_result = model_rnn.predict(df_data['text_test'])\n",
        "y_real = df_data['scaler'].inverse_transform(df_data['y_test'].reshape(-1, 1))\n",
        "y_predict_rnn = df_data['scaler'].inverse_transform(rnn_result.reshape(-1, 1))\n",
        "\n",
        "mean_absolute_error(y_real, y_predict_rnn)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 2s 114ms/step - loss: 0.1076 - mean_absolute_error: 0.0849\n",
            "Test Loss: 0.10764910280704498\n",
            "Test Accuracy: 0.08494479954242706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYStO92dER9G"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_score(y_real, y_predict_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdlFUl1KIPyq"
      },
      "source": [
        "def plot_regression_results(ax, y_true, y_pred, title, scores):\n",
        "    \"\"\"Scatter plot of the predicted vs true targets. \"\"\"\n",
        "    \n",
        "    ax.plot([y_true.min(), y_true.max()],\n",
        "            [y_true.min(), y_true.max()],\n",
        "            '--r', linewidth=2)\n",
        "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
        "\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines['left'].set_position(('outward', 10))\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "    ax.set_xlim([y_true.min(), y_true.max()])\n",
        "    ax.set_ylim([y_true.min(), y_true.max()])\n",
        "    ax.set_xlabel('Measured')\n",
        "    ax.set_ylabel('Predicted')\n",
        "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
        "                          edgecolor='none', linewidth=0)\n",
        "    ax.legend([extra], [scores], loc='upper left')\n",
        "    title = title\n",
        "    ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-BvtUYuIT2S"
      },
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=(9, 7))\n",
        "axs = np.ravel(axs)\n",
        "\n",
        "for ax, name in zip(axs, list(estimators.keys())):\n",
        "    \n",
        "    plot_regression_results(\n",
        "        ax, y_real, estimators[name]['y_predict'],\n",
        "        name,\n",
        "        (r'r2_score={:.2f}' + '\\n' + r'mae={:.2f}')\n",
        "        .format(estimators[name]['r2_score'],\n",
        "                estimators[name]['mae']))\n",
        "\n",
        "plt.suptitle('Predictors comparison ')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}