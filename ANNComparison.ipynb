{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNComparison.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ+F7C0QPZfchqFhnnAbAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/WineReviews/blob/main/ANNComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUlG4dnFJpJe",
        "outputId": "3a464851-db77-4b78-9e60-2b4e972cda22"
      },
      "source": [
        "!git clone -l -s https://github.com/mateopolancecofficial/WineReviews.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WineReviews'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 15), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WumCKdnHJtJx",
        "outputId": "6527ee98-3945-4fe4-e8bc-a9e8299a158c"
      },
      "source": [
        "pip install -q -U tensorflow-text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 7.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn8lmDLbJzVp",
        "outputId": "193882c4-863b-4435-836e-0c2d8b8f8ed0"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNcjJv-cJ1Pn",
        "outputId": "70b7b7e0-68a7-43fb-f2d9-41d8884b0122"
      },
      "source": [
        "pip install -q -U tf-models-official"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 79kB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 45.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 50.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 55.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HqgoB1BJ9xx"
      },
      "source": [
        "## DNN regressors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCzkdzq7J3fH"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Input\n",
        ")\n",
        "import kerastuner as kt\n",
        "from official.nlp import optimization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4T10XKqKD1P",
        "outputId": "73c06ea1-1a9a-4704-d4c0-4c2f55b92095"
      },
      "source": [
        "import os\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9Kqg6cfKMWA"
      },
      "source": [
        "### Load dataset and set global parameters\n",
        "\n",
        "Before text encoding and generating worde embeddings we need to load dataset, make train, validation and test splits and load it into tensorflow dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFeGkAKzKM2n"
      },
      "source": [
        "# set parameters\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "path_v1 = \"/content/WineReviews/Data/winemag-data-130k-v2.csv\" \n",
        "path_v2 = \"/content/WineReviews/Data/winemag-data_first150k.csv\"\n",
        "batch_size = 2048\n",
        "col_idx = 0\n",
        "train_size, test_size, val_size = 0.8, 0.2, 0.2\n",
        "transform = 'normalize'\n",
        "columns = ['description', 'points']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-IAuy9GKQnP"
      },
      "source": [
        "def load_data(path_v1: str, path_v2: str, columns: list, col_idx: int):\n",
        "  \"\"\"\n",
        "  Load and concatenate two datasets with removing duplicates.\n",
        "  param path_v1: import path of first dataset\n",
        "  param path_v2: import path of second dataset\n",
        "  param columns: list of columns to preserve in dataframe\n",
        "  param col_idx: index of column given in input columns list \n",
        "                 on which look for duplicates in dataframe\n",
        "  return:        pandas dataframe\n",
        "  \"\"\"\n",
        "  \n",
        "  df_v1 = pd.read_csv(path_v1, index_col=0)\n",
        "  df_v1 = df_v1[columns]\n",
        "  # remove numbers form column description from first dataframe\n",
        "  df_v1.description = df_v1.description.str.replace('\\d+', '')\n",
        "\n",
        "  df_v2 = pd.read_csv(path_v2, index_col=0)\n",
        "  df_v2 = df_v2[columns]\n",
        "  # remove numbers form column description from second dataframe\n",
        "  df_v2.description = df_v2.description.str.replace('\\d+', '')\n",
        "\n",
        "  df = pd.concat([df_v1, df_v2])\n",
        "\n",
        "  # dropping duplicte values\n",
        "  df.drop_duplicates(subset = columns[col_idx],\n",
        "                       keep = 'first', inplace = True)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-4e-kv_KSUv"
      },
      "source": [
        "def split_data(df: pd.DataFrame, train_size: float, test_size: float, \n",
        "               val_size: float):\n",
        "  \"\"\"\n",
        "  Split dataset on train, test and validation subsets.\n",
        "  param df:          input dataframe\n",
        "  param train_size:  fraction of train size\n",
        "  param test_size:   fraction of test size\n",
        "  param val_size:    fraction of validation size\n",
        "  return:            dictionary, keys=names of dataframes, columns=dataframes\n",
        "  \"\"\"\n",
        "  \n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac = 1)\n",
        "  \n",
        "  # split on test and train set\n",
        "  text_train, text_test, y_train, y_test = train_test_split(df.description, df.points,\n",
        "                                           test_size=test_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_test = y_train.astype('float'), y_test.astype('float')\n",
        "  \n",
        "  # split train set on train and validation subsets\n",
        "  text_train, text_val, y_train, y_val = train_test_split(text_train, y_train,\n",
        "                                                  test_size=val_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_val = y_train.astype('float'), y_val.astype('float')\n",
        "\n",
        "  y_train = y_train.to_numpy().reshape(-1, 1)\n",
        "  y_val = y_val.to_numpy().reshape(-1, 1)\n",
        "  y_test = y_test.to_numpy().reshape(-1, 1)\n",
        "\n",
        "  # apply target variable transformation\n",
        "  if transform == 'normalize':\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "    \n",
        "  elif transform == 'standardize':\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "    \n",
        "  else:\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train)\n",
        "    y_train = scaler.transform(y_train)\n",
        "    y_val = scaler.transform(y_val)\n",
        "    y_test = scaler.transform(y_test)\n",
        "  \n",
        "  y_train, y_val, y_test = y_train.ravel(), y_val.ravel(), y_test.ravel()\n",
        "\n",
        "  dataset_dict = {\n",
        "      'text_train': text_train,\n",
        "      'y_train': y_train,\n",
        "      'text_val': text_val,\n",
        "      'y_val': y_val,\n",
        "      'text_test': text_test,\n",
        "      'y_test': y_test,\n",
        "      'scaler': scaler\n",
        "  }\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rLlSmgFKT9I"
      },
      "source": [
        "def create_input_datasets(df_data: dict):\n",
        "  \"\"\"\n",
        "  Create tensorflow datasets based on input dataframes for train, validation \n",
        "  and test subsets.\n",
        "  param df_data: dictionary, keys=names of dataframes, columns=dataframes\n",
        "  return:        dictionary, keys=names of datasets, columns=datasets\n",
        "  \"\"\"\n",
        "\n",
        "  # create train dataset for input in tensorflow model\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_train'], \n",
        "                                                      df_data['y_train']))\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  train_ds = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_val'], \n",
        "                                                    df_data['y_val']))\n",
        "  val_dataset = val_dataset.batch(batch_size)\n",
        "  val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_test'], \n",
        "                                                     df_data['y_test']))\n",
        "  test_dataset = test_dataset.batch(batch_size)\n",
        "  test_ds = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  datasets = {\n",
        "      'train_dataset': train_dataset,\n",
        "      'val_dataset': val_dataset,\n",
        "      'test_dataset': test_dataset\n",
        "  } \n",
        "\n",
        "  return datasets"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Umx3EmCKV8f"
      },
      "source": [
        "# call data transformation functions\n",
        "df = load_data(path_v1, path_v2, columns, col_idx)\n",
        "df_data = split_data(df, train_size, test_size, val_size)\n",
        "datasets = create_input_datasets(df_data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDciZJ4LCOE"
      },
      "source": [
        "# define loss functions\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def rmse():\n",
        "  def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "  return root_mean_squared_error\n",
        "\n",
        "def rmsle():\n",
        "  def root_mean_squared_log_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(K.log(1+y_pred) - K.log(1+y_true))))\n",
        "  return root_mean_squared_log_error"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF99y_kfKwV-"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB19Jy66KYPP"
      },
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(datasets['train_dataset'].map(lambda text, label: text))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D_nHOOKLGAA"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def rnn_model_builder(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(encoder)\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,  return_sequences=True)))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "  hp_units_1 = hp.Int('units_1', min_value=64, max_value=128, step=16)\n",
        "  activation=hp.Choice(\n",
        "        'dense_activation',\n",
        "        values=['relu', 'tanh', 'sigmoid'],\n",
        "        default='relu'\n",
        "    )\n",
        "  model.add(Dense(units=hp_units_1, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_1',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  hp_units_2 = hp.Int('units_2', min_value=8, max_value=64, step=16)\n",
        "  model.add(Dense(units=hp_units_2, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_2',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "              loss=rmse(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjX2FlZQLKAg"
      },
      "source": [
        "# define early stop callback to prevent overfitting\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB7tljGeLP3g"
      },
      "source": [
        "rnn_tuner = kt.Hyperband(rnn_model_builder,\n",
        "                         objective='mean_absolute_error',\n",
        "                         max_epochs=5,\n",
        "                         directory='RNN'\n",
        "                         )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV92HLG7LVlq",
        "outputId": "dfdcf24e-2ba0-42b4-9c73-28463cef5557"
      },
      "source": [
        "rnn_tuner.search(df_data['text_train'], df_data['y_train'], \n",
        "                 validation_data=(df_data['text_val'], df_data['y_val']), \n",
        "                 epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# get optimal hyperparameters\n",
        "rnn_best_hps = rnn_tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 04m 04s]\n",
            "mean_absolute_error: 0.0837310254573822\n",
            "\n",
            "Best mean_absolute_error So Far: 0.07390762865543365\n",
            "Total elapsed time: 00h 08m 40s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units_1           |128               |112               \n",
            "dense_activation  |relu              |tanh              \n",
            "dropout_1         |0.3               |0.3               \n",
            "units_2           |56                |24                \n",
            "dropout_2         |0.5               |0.4               \n",
            "learning_rate     |0.001             |0.001             \n",
            "tuner/epochs      |2                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |1                 |1                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/2\n",
            "2977/3389 [=========================>....] - ETA: 12s - loss: 0.1247 - mean_absolute_error: 0.0996"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T63yNRidLV0Q"
      },
      "source": [
        "rnn_model = rnn_tuner.hypermodel.build(rnn_best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcJxi_-tLWAk"
      },
      "source": [
        "# checkpoints callback is not defined because a lack of disk space on Google Colab\n",
        "rnn_history = rnn_model.fit(datasets['train_dataset'],\n",
        "                            validation_data=datasets['val_dataset'],\n",
        "                            epochs=100,\n",
        "                            callbacks=[stop_early]\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70cWt-hyNe0V"
      },
      "source": [
        "### Use a pre-trained text embedding as the first layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkaUXR9dO_B8"
      },
      "source": [
        "For this example we will use a pre-trained text embedding model from TensorFlow Hub called \n",
        "google/nnlm-en-dim128-with-normalization/2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Ykty3pNanx"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def tf_model_builder(hp):\n",
        "  # use pretrained embeddings for input layer\n",
        "  hub_model = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
        "  # 'trainable=True' - boolean controlling whether this layer is trainable\n",
        "  hub_layer = hub.KerasLayer(hub_model, input_shape=[], dtype=tf.string, \n",
        "                             trainable=True)\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(hub_layer)\n",
        "  hp_units_1 = hp.Int('units_1', min_value=64, max_value=128, step=16)\n",
        "  activation=hp.Choice(\n",
        "        'dense_activation',\n",
        "        values=['relu', 'tanh', 'sigmoid'],\n",
        "        default='relu'\n",
        "    )\n",
        "  model.add(Dense(units=hp_units_1, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_1',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  hp_units_2 = hp.Int('units_2', min_value=8, max_value=64, step=16)\n",
        "  model.add(Dense(units=hp_units_2, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_2',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "              loss=rmse(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dsuqvdeNa2Y"
      },
      "source": [
        "# define early stop callback to prevent overfitting\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhDLgpHxNbQO"
      },
      "source": [
        "tf_tuner = kt.Hyperband(tf_model_builder,\n",
        "                        objective='mean_absolute_error',\n",
        "                        max_epochs=5,\n",
        "                        directory='TF'\n",
        "                       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwxXf3NsQC_f"
      },
      "source": [
        "tf_tuner.search(df_data['text_train'], df_data['y_train'], \n",
        "                validation_data=(df_data['text_val'], df_data['y_val']), \n",
        "                epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "tf_best_hps = tf_tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exAualU-QDKz"
      },
      "source": [
        "tf_model = tf_tuner.hypermodel.build(tf_best_hps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrBzMAYLQDWh"
      },
      "source": [
        "# checkpoints callback is not defined because a lack of disk space on Google Colab\n",
        "tf_history = tf_model.fit(datasets['train_dataset'],\n",
        "                          validation_data=datasets['val_dataset'],\n",
        "                          epochs=100,\n",
        "                          callbacks=[stop_early]\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-vbxmnQ42_"
      },
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWysgKqoQs8i"
      },
      "source": [
        "y_real = df_data['scaler'].inverse_transform(df_data['y_test'].reshape(-1, 1))\n",
        "\n",
        "rnn_result = rnn_model.predict(df_data['text_test'])\n",
        "y_predict_rnn = df_data['scaler'].inverse_transform(rnn_result.reshape(-1, 1))\n",
        "\n",
        "tf_result = tf_model.predict(df_data['text_test'])\n",
        "y_predict_tf = df_data['scaler'].inverse_transform(tf_result.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFVjqaS7QtKz"
      },
      "source": [
        "estimators = {'RNN': \n",
        "              {\n",
        "                  'mae': mean_absolute_error(y_real, y_predict_rnn),\n",
        "                  'r2_score': r2_score(y_real, y_predict_rnn),\n",
        "                  'y_predict': y_predict_rnn\n",
        "              },\n",
        "              'TF': \n",
        "              {\n",
        "                  'mae': mean_absolute_error(y_real, y_predict_tf),\n",
        "                  'r2_score': r2_score(y_real, y_predict_tf),\n",
        "                  'y_predict': y_predict_tf\n",
        "              }\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMl588MKQtYG"
      },
      "source": [
        "def plot_regression_results(ax, y_true, y_pred, title, scores):\n",
        "    \"\"\"Scatter plot of the predicted vs true targets. \"\"\"\n",
        "    \n",
        "    ax.plot([y_true.min(), y_true.max()],\n",
        "            [y_true.min(), y_true.max()],\n",
        "            '--r', linewidth=2)\n",
        "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
        "\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines['left'].set_position(('outward', 10))\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "    ax.set_xlim([y_true.min(), y_true.max()])\n",
        "    ax.set_ylim([y_true.min(), y_true.max()])\n",
        "    ax.set_xlabel('Measured')\n",
        "    ax.set_ylabel('Predicted')\n",
        "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
        "                          edgecolor='none', linewidth=0)\n",
        "    ax.legend([extra], [scores], loc='upper left')\n",
        "    title = title\n",
        "    ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xohbv387RlnB"
      },
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=(9, 7))\n",
        "axs = np.ravel(axs)\n",
        "\n",
        "for ax, name in zip(axs, list(estimators.keys())):\n",
        "    \n",
        "    plot_regression_results(\n",
        "        ax, y_real, estimators[name]['y_predict'],\n",
        "        name,\n",
        "        (r'r2_score={:.2f}' + '\\n' + r'mae={:.2f}')\n",
        "        .format(estimators[name]['r2_score'],\n",
        "                estimators[name]['mae']))\n",
        "\n",
        "plt.suptitle('Predictors comparison ')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}